{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Activation, Dropout,BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from keras.callbacks import TensorBoard\n",
    "#%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Livertable.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b =  range(473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data = data[data.columns[1:]].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data[0:10,1:10])\n",
    "data = data[0:, 1:]\n",
    "np.max(data)\n",
    "data = data /51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 15856)\n",
      "X_train shape :  (378, 15856)\n",
      "x_test shape :  (95, 15856)\n",
      "patients shape :  (473, 15856)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = data[:,:]\n",
    "print(np.shape(inputs))\n",
    "inputs = inputs.astype('float32')\n",
    "patients = inputs[:,:] \n",
    "patients1 = inputs[:,:]\n",
    "X_train, X_test,y,y = train_test_split(inputs,b,test_size=0.2 , random_state=42)\n",
    "\n",
    "print(\"X_train shape : \",np.shape(X_train))\n",
    "print(\"x_test shape : \",np.shape(X_test))\n",
    "print(\"patients shape : \",np.shape(patients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (378, 1, 15856)\n",
      "x_test shape :  (95, 1, 15856)\n",
      "patients shape :  (473, 1, 15856)\n"
     ]
    }
   ],
   "source": [
    "X_train =np.reshape(X_train, [-1, 1, 20159])\n",
    "X_test = np.reshape(X_test ,[-1, 1, 20159])\n",
    "patients = np.reshape(patients ,[-1, 1, 20159])\n",
    "\n",
    "print(\"X_train shape : \",np.shape(X_train))\n",
    "print(\"x_test shape : \",np.shape(X_test))\n",
    "print(\"patients shape : \",np.shape(patients))\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape = (1, 20159))\n",
    "\n",
    "\n",
    "x = Dense(1000,kernel_initializer='glorot_uniform', activation = 'relu')(input)\n",
    "x = BatchNormalization()(x)\n",
    "bottleneck = Dense(3, kernel_initializer='glorot_uniform', activation = 'relu', name='bottleneck')(x)\n",
    "x = Dense(3, kernel_initializer='glorot_uniform', activation = 'relu')(bottleneck)\n",
    "x = Dense(1000, kernel_initializer='glorot_uniform', activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Dense(20159, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model(input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.00005)\n",
    "sgd = keras.optimizers.SGD(lr=0.005,decay=10e-6,momentum=0.9, nesterov=1)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 378 samples, validate on 95 samples\n",
      "Epoch 1/10000\n",
      "378/378 [==============================] - 2s 6ms/step - loss: 0.2249 - val_loss: 0.2256\n",
      "Epoch 2/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 0.2245 - val_loss: 0.2254\n",
      "Epoch 3/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 0.2243 - val_loss: 0.2253\n",
      "Epoch 4/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.2242 - val_loss: 0.2252\n",
      "Epoch 5/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.2240 - val_loss: 0.2250\n",
      "Epoch 6/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.2238 - val_loss: 0.2248\n",
      "Epoch 7/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 0.2235 - val_loss: 0.2245\n",
      "Epoch 8/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 0.2232 - val_loss: 0.2242\n",
      "Epoch 9/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 0.2229 - val_loss: 0.2237\n",
      "Epoch 10/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.2224 - val_loss: 0.2232\n",
      "Epoch 11/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 0.2217 - val_loss: 0.2224\n",
      "Epoch 12/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.2209 - val_loss: 0.2215\n",
      "Epoch 13/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 0.2199 - val_loss: 0.2204\n",
      "Epoch 14/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 0.2187 - val_loss: 0.2191\n",
      "Epoch 15/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.2173 - val_loss: 0.2175\n",
      "Epoch 16/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 0.2156 - val_loss: 0.2156\n",
      "Epoch 17/10000\n",
      "378/378 [==============================] - 0s 959us/step - loss: 0.2136 - val_loss: 0.2134\n",
      "Epoch 18/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 0.2113 - val_loss: 0.2110\n",
      "Epoch 19/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 0.2087 - val_loss: 0.2082\n",
      "Epoch 20/10000\n",
      "378/378 [==============================] - 0s 948us/step - loss: 0.2058 - val_loss: 0.2052\n",
      "Epoch 21/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 0.2026 - val_loss: 0.2018\n",
      "Epoch 22/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 0.1992 - val_loss: 0.1982\n",
      "Epoch 23/10000\n",
      "378/378 [==============================] - 0s 907us/step - loss: 0.1954 - val_loss: 0.1942\n",
      "Epoch 24/10000\n",
      "378/378 [==============================] - 0s 908us/step - loss: 0.1913 - val_loss: 0.1900\n",
      "Epoch 25/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 0.1870 - val_loss: 0.1855\n",
      "Epoch 26/10000\n",
      "378/378 [==============================] - 0s 994us/step - loss: 0.1824 - val_loss: 0.1808\n",
      "Epoch 27/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 0.1776 - val_loss: 0.1759\n",
      "Epoch 28/10000\n",
      "378/378 [==============================] - 0s 959us/step - loss: 0.1726 - val_loss: 0.1708\n",
      "Epoch 29/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.1674 - val_loss: 0.1654\n",
      "Epoch 30/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 0.1620 - val_loss: 0.1600\n",
      "Epoch 31/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 0.1565 - val_loss: 0.1544\n",
      "Epoch 32/10000\n",
      "378/378 [==============================] - 0s 941us/step - loss: 0.1509 - val_loss: 0.1488\n",
      "Epoch 33/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 0.1453 - val_loss: 0.1431\n",
      "Epoch 34/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.1396 - val_loss: 0.1374\n",
      "Epoch 35/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 0.1339 - val_loss: 0.1317\n",
      "Epoch 36/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 0.1282 - val_loss: 0.1260\n",
      "Epoch 37/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 0.1225 - val_loss: 0.1203\n",
      "Epoch 38/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 0.1170 - val_loss: 0.1148\n",
      "Epoch 39/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 0.1115 - val_loss: 0.1093\n",
      "Epoch 40/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.1061 - val_loss: 0.1040\n",
      "Epoch 41/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.1009 - val_loss: 0.0989\n",
      "Epoch 42/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 0.0958 - val_loss: 0.0939\n",
      "Epoch 43/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 0.0909 - val_loss: 0.0890\n",
      "Epoch 44/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0861 - val_loss: 0.0844\n",
      "Epoch 45/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 0.0816 - val_loss: 0.0799\n",
      "Epoch 46/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 0.0772 - val_loss: 0.0756\n",
      "Epoch 47/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 0.0730 - val_loss: 0.0715\n",
      "Epoch 48/10000\n",
      "378/378 [==============================] - 0s 948us/step - loss: 0.0691 - val_loss: 0.0676\n",
      "Epoch 49/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0653 - val_loss: 0.0639\n",
      "Epoch 50/10000\n",
      "378/378 [==============================] - 0s 969us/step - loss: 0.0617 - val_loss: 0.0604\n",
      "Epoch 51/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 0.0583 - val_loss: 0.0571\n",
      "Epoch 52/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 0.0551 - val_loss: 0.0540\n",
      "Epoch 53/10000\n",
      "378/378 [==============================] - 0s 926us/step - loss: 0.0520 - val_loss: 0.0510\n",
      "Epoch 54/10000\n",
      "378/378 [==============================] - 0s 918us/step - loss: 0.0492 - val_loss: 0.0482\n",
      "Epoch 55/10000\n",
      "378/378 [==============================] - 0s 945us/step - loss: 0.0465 - val_loss: 0.0456\n",
      "Epoch 56/10000\n",
      "378/378 [==============================] - 0s 930us/step - loss: 0.0439 - val_loss: 0.0431\n",
      "Epoch 57/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0415 - val_loss: 0.0408\n",
      "Epoch 58/10000\n",
      "378/378 [==============================] - 0s 998us/step - loss: 0.0393 - val_loss: 0.0386\n",
      "Epoch 59/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0372 - val_loss: 0.0365\n",
      "Epoch 60/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 0.0352 - val_loss: 0.0346\n",
      "Epoch 61/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 0.0333 - val_loss: 0.0328\n",
      "Epoch 62/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 0.0316 - val_loss: 0.0311\n",
      "Epoch 63/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 0.0299 - val_loss: 0.0295\n",
      "Epoch 64/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 0.0284 - val_loss: 0.0280\n",
      "Epoch 65/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 0.0270 - val_loss: 0.0266\n",
      "Epoch 66/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 0.0256 - val_loss: 0.0253\n",
      "Epoch 67/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 0.0243 - val_loss: 0.0241\n",
      "Epoch 68/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 0.0231 - val_loss: 0.0229\n",
      "Epoch 69/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 0.0220 - val_loss: 0.0218\n",
      "Epoch 70/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 0.0210 - val_loss: 0.0208\n",
      "Epoch 71/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 0.0200 - val_loss: 0.0198\n",
      "Epoch 72/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 73/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 0.0182 - val_loss: 0.0180\n",
      "Epoch 74/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 75/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 76/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 77/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.0151 - val_loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0145 - val_loss: 0.0144\n",
      "Epoch 79/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 0.0139 - val_loss: 0.0138\n",
      "Epoch 80/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 81/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 82/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 83/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 84/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 85/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 86/10000\n",
      "378/378 [==============================] - 0s 906us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 87/10000\n",
      "378/378 [==============================] - 0s 924us/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 88/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 89/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 90/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 91/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 92/10000\n",
      "378/378 [==============================] - 0s 948us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 93/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 94/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 95/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 96/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 97/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 98/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 99/10000\n",
      "378/378 [==============================] - 0s 932us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 100/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 101/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 102/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 103/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 104/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 105/10000\n",
      "378/378 [==============================] - 0s 919us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 106/10000\n",
      "378/378 [==============================] - 0s 937us/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 107/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 108/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 109/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 110/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 111/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 112/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 113/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 114/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 115/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 116/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 117/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 118/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 119/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 120/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 121/10000\n",
      "378/378 [==============================] - 0s 995us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 122/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 123/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 124/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 125/10000\n",
      "378/378 [==============================] - 0s 943us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 126/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 127/10000\n",
      "378/378 [==============================] - 0s 997us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 128/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 129/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 130/10000\n",
      "378/378 [==============================] - 0s 997us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 131/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 132/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 133/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 134/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 135/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 136/10000\n",
      "378/378 [==============================] - 0s 938us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 137/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 138/10000\n",
      "378/378 [==============================] - 0s 969us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 139/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 140/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 141/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 142/10000\n",
      "378/378 [==============================] - 0s 969us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 143/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 144/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 145/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 146/10000\n",
      "378/378 [==============================] - 0s 996us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 147/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 148/10000\n",
      "378/378 [==============================] - 0s 937us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 149/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 150/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 151/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 152/10000\n",
      "378/378 [==============================] - 0s 997us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 153/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 154/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 0.0017 - val_loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 156/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 157/10000\n",
      "378/378 [==============================] - 0s 995us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 158/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 159/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 160/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 161/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 162/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 163/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 164/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 165/10000\n",
      "378/378 [==============================] - 0s 924us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 166/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 167/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 168/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 169/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 170/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 171/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 172/10000\n",
      "378/378 [==============================] - 0s 938us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 173/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 174/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 175/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 176/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 177/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 178/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 179/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 180/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 181/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 182/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 183/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 184/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 185/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 186/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 187/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 188/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 189/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 9.9620e-04 - val_loss: 0.0010\n",
      "Epoch 190/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 9.8435e-04 - val_loss: 0.0010\n",
      "Epoch 191/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 9.7275e-04 - val_loss: 0.0010\n",
      "Epoch 192/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 9.6154e-04 - val_loss: 9.9644e-04\n",
      "Epoch 193/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 9.5038e-04 - val_loss: 9.8498e-04\n",
      "Epoch 194/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 9.3958e-04 - val_loss: 9.7382e-04\n",
      "Epoch 195/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 9.2910e-04 - val_loss: 9.6280e-04\n",
      "Epoch 196/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 9.1885e-04 - val_loss: 9.5190e-04\n",
      "Epoch 197/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 9.0879e-04 - val_loss: 9.4124e-04\n",
      "Epoch 198/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 8.9870e-04 - val_loss: 9.3112e-04\n",
      "Epoch 199/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 8.8951e-04 - val_loss: 9.2069e-04\n",
      "Epoch 200/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 8.7984e-04 - val_loss: 9.1085e-04\n",
      "Epoch 201/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 8.7066e-04 - val_loss: 9.0126e-04\n",
      "Epoch 202/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 8.6181e-04 - val_loss: 8.9176e-04\n",
      "Epoch 203/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 8.5302e-04 - val_loss: 8.8259e-04\n",
      "Epoch 204/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 8.4449e-04 - val_loss: 8.7366e-04\n",
      "Epoch 205/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 8.3606e-04 - val_loss: 8.6502e-04\n",
      "Epoch 206/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 8.2799e-04 - val_loss: 8.5638e-04\n",
      "Epoch 207/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 8.2007e-04 - val_loss: 8.4779e-04\n",
      "Epoch 208/10000\n",
      "378/378 [==============================] - 0s 930us/step - loss: 8.1206e-04 - val_loss: 8.3963e-04\n",
      "Epoch 209/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 8.0443e-04 - val_loss: 8.3160e-04\n",
      "Epoch 210/10000\n",
      "378/378 [==============================] - 0s 912us/step - loss: 7.9718e-04 - val_loss: 8.2344e-04\n",
      "Epoch 211/10000\n",
      "378/378 [==============================] - 0s 936us/step - loss: 7.8969e-04 - val_loss: 8.1566e-04\n",
      "Epoch 212/10000\n",
      "378/378 [==============================] - 0s 927us/step - loss: 7.8246e-04 - val_loss: 8.0818e-04\n",
      "Epoch 213/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 7.7543e-04 - val_loss: 8.0087e-04\n",
      "Epoch 214/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 7.6872e-04 - val_loss: 7.9353e-04\n",
      "Epoch 215/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 7.6195e-04 - val_loss: 7.8642e-04\n",
      "Epoch 216/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 7.5545e-04 - val_loss: 7.7938e-04\n",
      "Epoch 217/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 7.4880e-04 - val_loss: 7.7280e-04\n",
      "Epoch 218/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 7.4280e-04 - val_loss: 7.6595e-04\n",
      "Epoch 219/10000\n",
      "378/378 [==============================] - 0s 996us/step - loss: 7.3647e-04 - val_loss: 7.5946e-04\n",
      "Epoch 220/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 7.3058e-04 - val_loss: 7.5295e-04\n",
      "Epoch 221/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 7.2465e-04 - val_loss: 7.4668e-04\n",
      "Epoch 222/10000\n",
      "378/378 [==============================] - 0s 995us/step - loss: 7.1881e-04 - val_loss: 7.4061e-04\n",
      "Epoch 223/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 7.1323e-04 - val_loss: 7.3454e-04\n",
      "Epoch 224/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 7.0772e-04 - val_loss: 7.2856e-04\n",
      "Epoch 225/10000\n",
      "378/378 [==============================] - 0s 919us/step - loss: 7.0224e-04 - val_loss: 7.2282e-04\n",
      "Epoch 226/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 6.9702e-04 - val_loss: 7.1708e-04\n",
      "Epoch 227/10000\n",
      "378/378 [==============================] - 0s 919us/step - loss: 6.9182e-04 - val_loss: 7.1148e-04\n",
      "Epoch 228/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 6.8664e-04 - val_loss: 7.0614e-04\n",
      "Epoch 229/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 6.8174e-04 - val_loss: 7.0085e-04\n",
      "Epoch 230/10000\n",
      "378/378 [==============================] - 0s 999us/step - loss: 6.7700e-04 - val_loss: 6.9554e-04\n",
      "Epoch 231/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 6.7208e-04 - val_loss: 6.9055e-04\n",
      "Epoch 232/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 6.6740e-04 - val_loss: 6.8563e-04\n",
      "Epoch 233/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 6.6303e-04 - val_loss: 6.8051e-04\n",
      "Epoch 234/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 6.5832e-04 - val_loss: 6.7577e-04\n",
      "Epoch 235/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 6.5399e-04 - val_loss: 6.7100e-04\n",
      "Epoch 236/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 6.4979e-04 - val_loss: 6.6621e-04\n",
      "Epoch 237/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 6.4548e-04 - val_loss: 6.6167e-04\n",
      "Epoch 238/10000\n",
      "378/378 [==============================] - 0s 932us/step - loss: 6.4137e-04 - val_loss: 6.5721e-04\n",
      "Epoch 239/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 6.3728e-04 - val_loss: 6.5289e-04\n",
      "Epoch 240/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 6.3338e-04 - val_loss: 6.4855e-04\n",
      "Epoch 241/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 6.2941e-04 - val_loss: 6.4439e-04\n",
      "Epoch 242/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 6.2570e-04 - val_loss: 6.4021e-04\n",
      "Epoch 243/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 6.2195e-04 - val_loss: 6.3615e-04\n",
      "Epoch 244/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 6.1824e-04 - val_loss: 6.3225e-04\n",
      "Epoch 245/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 6.1467e-04 - val_loss: 6.2839e-04\n",
      "Epoch 246/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 6.1116e-04 - val_loss: 6.2459e-04\n",
      "Epoch 247/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 6.0778e-04 - val_loss: 6.2080e-04\n",
      "Epoch 248/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 6.0436e-04 - val_loss: 6.1716e-04\n",
      "Epoch 249/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 6.0119e-04 - val_loss: 6.1344e-04\n",
      "Epoch 250/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 5.9786e-04 - val_loss: 6.0995e-04\n",
      "Epoch 251/10000\n",
      "378/378 [==============================] - 0s 927us/step - loss: 5.9480e-04 - val_loss: 6.0640e-04\n",
      "Epoch 252/10000\n",
      "378/378 [==============================] - 0s 896us/step - loss: 5.9158e-04 - val_loss: 6.0307e-04\n",
      "Epoch 253/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 5.8867e-04 - val_loss: 5.9963e-04\n",
      "Epoch 254/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 5.8550e-04 - val_loss: 5.9651e-04\n",
      "Epoch 255/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 5.8266e-04 - val_loss: 5.9331e-04\n",
      "Epoch 256/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 5.7988e-04 - val_loss: 5.9005e-04\n",
      "Epoch 257/10000\n",
      "378/378 [==============================] - 0s 959us/step - loss: 5.7694e-04 - val_loss: 5.8702e-04\n",
      "Epoch 258/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 5.7422e-04 - val_loss: 5.8398e-04\n",
      "Epoch 259/10000\n",
      "378/378 [==============================] - 0s 921us/step - loss: 5.7150e-04 - val_loss: 5.8102e-04\n",
      "Epoch 260/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 5.6889e-04 - val_loss: 5.7807e-04\n",
      "Epoch 261/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 5.6634e-04 - val_loss: 5.7512e-04\n",
      "Epoch 262/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 5.6366e-04 - val_loss: 5.7236e-04\n",
      "Epoch 263/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 5.6119e-04 - val_loss: 5.6960e-04\n",
      "Epoch 264/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 5.5878e-04 - val_loss: 5.6680e-04\n",
      "Epoch 265/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 5.5632e-04 - val_loss: 5.6415e-04\n",
      "Epoch 266/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 5.5392e-04 - val_loss: 5.6162e-04\n",
      "Epoch 267/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 5.5172e-04 - val_loss: 5.5896e-04\n",
      "Epoch 268/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 5.4940e-04 - val_loss: 5.5644e-04\n",
      "Epoch 269/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 5.4713e-04 - val_loss: 5.5404e-04\n",
      "Epoch 270/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 5.4502e-04 - val_loss: 5.5159e-04\n",
      "Epoch 271/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 5.4284e-04 - val_loss: 5.4927e-04\n",
      "Epoch 272/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 5.4085e-04 - val_loss: 5.4685e-04\n",
      "Epoch 273/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 5.3868e-04 - val_loss: 5.4464e-04\n",
      "Epoch 274/10000\n",
      "378/378 [==============================] - 0s 994us/step - loss: 5.3674e-04 - val_loss: 5.4237e-04\n",
      "Epoch 275/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 5.3473e-04 - val_loss: 5.4019e-04\n",
      "Epoch 276/10000\n",
      "378/378 [==============================] - 0s 931us/step - loss: 5.3291e-04 - val_loss: 5.3788e-04\n",
      "Epoch 277/10000\n",
      "378/378 [==============================] - 0s 932us/step - loss: 5.3084e-04 - val_loss: 5.3580e-04\n",
      "Epoch 278/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 5.2906e-04 - val_loss: 5.3364e-04\n",
      "Epoch 279/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 5.2719e-04 - val_loss: 5.3159e-04\n",
      "Epoch 280/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 5.2545e-04 - val_loss: 5.2952e-04\n",
      "Epoch 281/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 5.2362e-04 - val_loss: 5.2761e-04\n",
      "Epoch 282/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 5.2189e-04 - val_loss: 5.2571e-04\n",
      "Epoch 283/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 5.2025e-04 - val_loss: 5.2373e-04\n",
      "Epoch 284/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 5.1856e-04 - val_loss: 5.2182e-04\n",
      "Epoch 285/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 5.1694e-04 - val_loss: 5.1991e-04\n",
      "Epoch 286/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 5.1532e-04 - val_loss: 5.1809e-04\n",
      "Epoch 287/10000\n",
      "378/378 [==============================] - 0s 994us/step - loss: 5.1375e-04 - val_loss: 5.1629e-04\n",
      "Epoch 288/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 5.1222e-04 - val_loss: 5.1452e-04\n",
      "Epoch 289/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 5.1063e-04 - val_loss: 5.1288e-04\n",
      "Epoch 290/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 5.0926e-04 - val_loss: 5.1112e-04\n",
      "Epoch 291/10000\n",
      "378/378 [==============================] - 0s 997us/step - loss: 5.0776e-04 - val_loss: 5.0946e-04\n",
      "Epoch 292/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 5.0635e-04 - val_loss: 5.0780e-04\n",
      "Epoch 293/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 5.0492e-04 - val_loss: 5.0623e-04\n",
      "Epoch 294/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 5.0358e-04 - val_loss: 5.0464e-04\n",
      "Epoch 295/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 5.0221e-04 - val_loss: 5.0310e-04\n",
      "Epoch 296/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 5.0085e-04 - val_loss: 5.0164e-04\n",
      "Epoch 297/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 4.9960e-04 - val_loss: 5.0011e-04\n",
      "Epoch 298/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.9832e-04 - val_loss: 4.9858e-04\n",
      "Epoch 299/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 980us/step - loss: 4.9705e-04 - val_loss: 4.9713e-04\n",
      "Epoch 300/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.9577e-04 - val_loss: 4.9577e-04\n",
      "Epoch 301/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 4.9462e-04 - val_loss: 4.9432e-04\n",
      "Epoch 302/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 4.9340e-04 - val_loss: 4.9294e-04\n",
      "Epoch 303/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 4.9225e-04 - val_loss: 4.9155e-04\n",
      "Epoch 304/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.9107e-04 - val_loss: 4.9025e-04\n",
      "Epoch 305/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 4.9000e-04 - val_loss: 4.8890e-04\n",
      "Epoch 306/10000\n",
      "378/378 [==============================] - 0s 945us/step - loss: 4.8895e-04 - val_loss: 4.8752e-04\n",
      "Epoch 307/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.8776e-04 - val_loss: 4.8633e-04\n",
      "Epoch 308/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.8676e-04 - val_loss: 4.8511e-04\n",
      "Epoch 309/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 4.8577e-04 - val_loss: 4.8386e-04\n",
      "Epoch 310/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.8473e-04 - val_loss: 4.8270e-04\n",
      "Epoch 311/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.8372e-04 - val_loss: 4.8157e-04\n",
      "Epoch 312/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 4.8277e-04 - val_loss: 4.8044e-04\n",
      "Epoch 313/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.8184e-04 - val_loss: 4.7929e-04\n",
      "Epoch 314/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.8087e-04 - val_loss: 4.7821e-04\n",
      "Epoch 315/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 4.7998e-04 - val_loss: 4.7710e-04\n",
      "Epoch 316/10000\n",
      "378/378 [==============================] - 0s 998us/step - loss: 4.7910e-04 - val_loss: 4.7598e-04\n",
      "Epoch 317/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.7815e-04 - val_loss: 4.7497e-04\n",
      "Epoch 318/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.7736e-04 - val_loss: 4.7387e-04\n",
      "Epoch 319/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.7643e-04 - val_loss: 4.7289e-04\n",
      "Epoch 320/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.7564e-04 - val_loss: 4.7188e-04\n",
      "Epoch 321/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 4.7484e-04 - val_loss: 4.7086e-04\n",
      "Epoch 322/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.7399e-04 - val_loss: 4.6992e-04\n",
      "Epoch 323/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.7318e-04 - val_loss: 4.6902e-04\n",
      "Epoch 324/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 4.7247e-04 - val_loss: 4.6803e-04\n",
      "Epoch 325/10000\n",
      "378/378 [==============================] - 0s 943us/step - loss: 4.7169e-04 - val_loss: 4.6710e-04\n",
      "Epoch 326/10000\n",
      "378/378 [==============================] - 0s 920us/step - loss: 4.7087e-04 - val_loss: 4.6629e-04\n",
      "Epoch 327/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 4.7021e-04 - val_loss: 4.6536e-04\n",
      "Epoch 328/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.6945e-04 - val_loss: 4.6450e-04\n",
      "Epoch 329/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 4.6881e-04 - val_loss: 4.6357e-04\n",
      "Epoch 330/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 4.6805e-04 - val_loss: 4.6274e-04\n",
      "Epoch 331/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 4.6738e-04 - val_loss: 4.6191e-04\n",
      "Epoch 332/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 4.6672e-04 - val_loss: 4.6110e-04\n",
      "Epoch 333/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 4.6607e-04 - val_loss: 4.6028e-04\n",
      "Epoch 334/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 4.6540e-04 - val_loss: 4.5952e-04\n",
      "Epoch 335/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.6481e-04 - val_loss: 4.5873e-04\n",
      "Epoch 336/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 4.6416e-04 - val_loss: 4.5798e-04\n",
      "Epoch 337/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 4.6361e-04 - val_loss: 4.5718e-04\n",
      "Epoch 338/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 4.6297e-04 - val_loss: 4.5647e-04\n",
      "Epoch 339/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.6241e-04 - val_loss: 4.5574e-04\n",
      "Epoch 340/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.6184e-04 - val_loss: 4.5504e-04\n",
      "Epoch 341/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.6127e-04 - val_loss: 4.5434e-04\n",
      "Epoch 342/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.6072e-04 - val_loss: 4.5369e-04\n",
      "Epoch 343/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.6023e-04 - val_loss: 4.5297e-04\n",
      "Epoch 344/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 4.5966e-04 - val_loss: 4.5231e-04\n",
      "Epoch 345/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 4.5914e-04 - val_loss: 4.5169e-04\n",
      "Epoch 346/10000\n",
      "378/378 [==============================] - 0s 969us/step - loss: 4.5864e-04 - val_loss: 4.5106e-04\n",
      "Epoch 347/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 4.5812e-04 - val_loss: 4.5049e-04\n",
      "Epoch 348/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.5767e-04 - val_loss: 4.4984e-04\n",
      "Epoch 349/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.5717e-04 - val_loss: 4.4923e-04\n",
      "Epoch 350/10000\n",
      "378/378 [==============================] - 0s 994us/step - loss: 4.5670e-04 - val_loss: 4.4862e-04\n",
      "Epoch 351/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 4.5626e-04 - val_loss: 4.4798e-04\n",
      "Epoch 352/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 4.5576e-04 - val_loss: 4.4742e-04\n",
      "Epoch 353/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.5532e-04 - val_loss: 4.4686e-04\n",
      "Epoch 354/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 4.5492e-04 - val_loss: 4.4626e-04\n",
      "Epoch 355/10000\n",
      "378/378 [==============================] - 0s 934us/step - loss: 4.5447e-04 - val_loss: 4.4572e-04\n",
      "Epoch 356/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 4.5406e-04 - val_loss: 4.4517e-04\n",
      "Epoch 357/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 4.5366e-04 - val_loss: 4.4463e-04\n",
      "Epoch 358/10000\n",
      "378/378 [==============================] - 0s 1000us/step - loss: 4.5325e-04 - val_loss: 4.4412e-04\n",
      "Epoch 359/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.5285e-04 - val_loss: 4.4362e-04\n",
      "Epoch 360/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.5249e-04 - val_loss: 4.4310e-04\n",
      "Epoch 361/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.5209e-04 - val_loss: 4.4261e-04\n",
      "Epoch 362/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 4.5173e-04 - val_loss: 4.4212e-04\n",
      "Epoch 363/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.5136e-04 - val_loss: 4.4166e-04\n",
      "Epoch 364/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.5104e-04 - val_loss: 4.4117e-04\n",
      "Epoch 365/10000\n",
      "378/378 [==============================] - 0s 997us/step - loss: 4.5066e-04 - val_loss: 4.4072e-04\n",
      "Epoch 366/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 4.5030e-04 - val_loss: 4.4031e-04\n",
      "Epoch 367/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.5002e-04 - val_loss: 4.3982e-04\n",
      "Epoch 368/10000\n",
      "378/378 [==============================] - 0s 998us/step - loss: 4.4965e-04 - val_loss: 4.3939e-04\n",
      "Epoch 369/10000\n",
      "378/378 [==============================] - 0s 993us/step - loss: 4.4934e-04 - val_loss: 4.3895e-04\n",
      "Epoch 370/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.4902e-04 - val_loss: 4.3852e-04\n",
      "Epoch 371/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.4871e-04 - val_loss: 4.3811e-04\n",
      "Epoch 372/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.4839e-04 - val_loss: 4.3773e-04\n",
      "Epoch 373/10000\n",
      "378/378 [==============================] - 0s 998us/step - loss: 4.4813e-04 - val_loss: 4.3730e-04\n",
      "Epoch 374/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 4.4781e-04 - val_loss: 4.3692e-04\n",
      "Epoch 375/10000\n",
      "378/378 [==============================] - 0s 945us/step - loss: 4.4753e-04 - val_loss: 4.3654e-04\n",
      "Epoch 376/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.4726e-04 - val_loss: 4.3615e-04\n",
      "Epoch 377/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.4699e-04 - val_loss: 4.3576e-04\n",
      "Epoch 378/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.4674e-04 - val_loss: 4.3537e-04\n",
      "Epoch 379/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.4647e-04 - val_loss: 4.3501e-04\n",
      "Epoch 380/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.4619e-04 - val_loss: 4.3469e-04\n",
      "Epoch 381/10000\n",
      "378/378 [==============================] - 0s 930us/step - loss: 4.4597e-04 - val_loss: 4.3433e-04\n",
      "Epoch 382/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 4.4570e-04 - val_loss: 4.3404e-04\n",
      "Epoch 383/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.4548e-04 - val_loss: 4.3370e-04\n",
      "Epoch 384/10000\n",
      "378/378 [==============================] - 0s 939us/step - loss: 4.4525e-04 - val_loss: 4.3336e-04\n",
      "Epoch 385/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.4500e-04 - val_loss: 4.3306e-04\n",
      "Epoch 386/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 4.4480e-04 - val_loss: 4.3272e-04\n",
      "Epoch 387/10000\n",
      "378/378 [==============================] - 0s 994us/step - loss: 4.4460e-04 - val_loss: 4.3235e-04\n",
      "Epoch 388/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.4432e-04 - val_loss: 4.3209e-04\n",
      "Epoch 389/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.4414e-04 - val_loss: 4.3177e-04\n",
      "Epoch 390/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.4390e-04 - val_loss: 4.3149e-04\n",
      "Epoch 391/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 4.4371e-04 - val_loss: 4.3118e-04\n",
      "Epoch 392/10000\n",
      "378/378 [==============================] - 0s 928us/step - loss: 4.4351e-04 - val_loss: 4.3087e-04\n",
      "Epoch 393/10000\n",
      "378/378 [==============================] - 0s 940us/step - loss: 4.4331e-04 - val_loss: 4.3056e-04\n",
      "Epoch 394/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.4311e-04 - val_loss: 4.3027e-04\n",
      "Epoch 395/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 4.4291e-04 - val_loss: 4.3001e-04\n",
      "Epoch 396/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.4273e-04 - val_loss: 4.2975e-04\n",
      "Epoch 397/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 4.4257e-04 - val_loss: 4.2945e-04\n",
      "Epoch 398/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 4.4238e-04 - val_loss: 4.2919e-04\n",
      "Epoch 399/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 4.4219e-04 - val_loss: 4.2896e-04\n",
      "Epoch 400/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.4203e-04 - val_loss: 4.2871e-04\n",
      "Epoch 401/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.4187e-04 - val_loss: 4.2844e-04\n",
      "Epoch 402/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.4169e-04 - val_loss: 4.2820e-04\n",
      "Epoch 403/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 4.4154e-04 - val_loss: 4.2795e-04\n",
      "Epoch 404/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.4138e-04 - val_loss: 4.2772e-04\n",
      "Epoch 405/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.4122e-04 - val_loss: 4.2750e-04\n",
      "Epoch 406/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 4.4108e-04 - val_loss: 4.2727e-04\n",
      "Epoch 407/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.4094e-04 - val_loss: 4.2703e-04\n",
      "Epoch 408/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 4.4079e-04 - val_loss: 4.2679e-04\n",
      "Epoch 409/10000\n",
      "378/378 [==============================] - 0s 911us/step - loss: 4.4063e-04 - val_loss: 4.2662e-04\n",
      "Epoch 410/10000\n",
      "378/378 [==============================] - 0s 919us/step - loss: 4.4052e-04 - val_loss: 4.2639e-04\n",
      "Epoch 411/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.4037e-04 - val_loss: 4.2619e-04\n",
      "Epoch 412/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 4.4024e-04 - val_loss: 4.2599e-04\n",
      "Epoch 413/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 4.4012e-04 - val_loss: 4.2576e-04\n",
      "Epoch 414/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 4.3999e-04 - val_loss: 4.2555e-04\n",
      "Epoch 415/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 4.3986e-04 - val_loss: 4.2537e-04\n",
      "Epoch 416/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.3974e-04 - val_loss: 4.2518e-04\n",
      "Epoch 417/10000\n",
      "378/378 [==============================] - 0s 932us/step - loss: 4.3962e-04 - val_loss: 4.2499e-04\n",
      "Epoch 418/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 4.3951e-04 - val_loss: 4.2484e-04\n",
      "Epoch 419/10000\n",
      "378/378 [==============================] - 0s 960us/step - loss: 4.3940e-04 - val_loss: 4.2466e-04\n",
      "Epoch 420/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.3933e-04 - val_loss: 4.2443e-04\n",
      "Epoch 421/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 4.3920e-04 - val_loss: 4.2426e-04\n",
      "Epoch 422/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 4.3907e-04 - val_loss: 4.2411e-04\n",
      "Epoch 423/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3898e-04 - val_loss: 4.2396e-04\n",
      "Epoch 424/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 4.3888e-04 - val_loss: 4.2378e-04\n",
      "Epoch 425/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.3879e-04 - val_loss: 4.2361e-04\n",
      "Epoch 426/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.3868e-04 - val_loss: 4.2347e-04\n",
      "Epoch 427/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.3859e-04 - val_loss: 4.2333e-04\n",
      "Epoch 428/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 4.3851e-04 - val_loss: 4.2315e-04\n",
      "Epoch 429/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.3841e-04 - val_loss: 4.2301e-04\n",
      "Epoch 430/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.3832e-04 - val_loss: 4.2285e-04\n",
      "Epoch 431/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 4.3823e-04 - val_loss: 4.2269e-04\n",
      "Epoch 432/10000\n",
      "378/378 [==============================] - 0s 914us/step - loss: 4.3815e-04 - val_loss: 4.2254e-04\n",
      "Epoch 433/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 4.3806e-04 - val_loss: 4.2239e-04\n",
      "Epoch 434/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 4.3798e-04 - val_loss: 4.2225e-04\n",
      "Epoch 435/10000\n",
      "378/378 [==============================] - 0s 963us/step - loss: 4.3790e-04 - val_loss: 4.2214e-04\n",
      "Epoch 436/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 4.3783e-04 - val_loss: 4.2199e-04\n",
      "Epoch 437/10000\n",
      "378/378 [==============================] - 0s 933us/step - loss: 4.3776e-04 - val_loss: 4.2183e-04\n",
      "Epoch 438/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.3767e-04 - val_loss: 4.2171e-04\n",
      "Epoch 439/10000\n",
      "378/378 [==============================] - 0s 942us/step - loss: 4.3760e-04 - val_loss: 4.2159e-04\n",
      "Epoch 440/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 4.3754e-04 - val_loss: 4.2144e-04\n",
      "Epoch 441/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 940us/step - loss: 4.3747e-04 - val_loss: 4.2129e-04\n",
      "Epoch 442/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 4.3740e-04 - val_loss: 4.2117e-04\n",
      "Epoch 443/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 4.3732e-04 - val_loss: 4.2109e-04\n",
      "Epoch 444/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3726e-04 - val_loss: 4.2099e-04\n",
      "Epoch 445/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 4.3720e-04 - val_loss: 4.2087e-04\n",
      "Epoch 446/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.3717e-04 - val_loss: 4.2070e-04\n",
      "Epoch 447/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 4.3708e-04 - val_loss: 4.2058e-04\n",
      "Epoch 448/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 4.3701e-04 - val_loss: 4.2048e-04\n",
      "Epoch 449/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3697e-04 - val_loss: 4.2037e-04\n",
      "Epoch 450/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.3690e-04 - val_loss: 4.2027e-04\n",
      "Epoch 451/10000\n",
      "378/378 [==============================] - 0s 961us/step - loss: 4.3685e-04 - val_loss: 4.2017e-04\n",
      "Epoch 452/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 4.3680e-04 - val_loss: 4.2010e-04\n",
      "Epoch 453/10000\n",
      "378/378 [==============================] - 0s 944us/step - loss: 4.3675e-04 - val_loss: 4.1998e-04\n",
      "Epoch 454/10000\n",
      "378/378 [==============================] - 0s 943us/step - loss: 4.3670e-04 - val_loss: 4.1988e-04\n",
      "Epoch 455/10000\n",
      "378/378 [==============================] - 0s 954us/step - loss: 4.3665e-04 - val_loss: 4.1977e-04\n",
      "Epoch 456/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.3660e-04 - val_loss: 4.1969e-04\n",
      "Epoch 457/10000\n",
      "378/378 [==============================] - 0s 996us/step - loss: 4.3655e-04 - val_loss: 4.1960e-04\n",
      "Epoch 458/10000\n",
      "378/378 [==============================] - 0s 953us/step - loss: 4.3651e-04 - val_loss: 4.1949e-04\n",
      "Epoch 459/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 4.3649e-04 - val_loss: 4.1937e-04\n",
      "Epoch 460/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.3641e-04 - val_loss: 4.1931e-04\n",
      "Epoch 461/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 4.3637e-04 - val_loss: 4.1921e-04\n",
      "Epoch 462/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 4.3633e-04 - val_loss: 4.1912e-04\n",
      "Epoch 463/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.3629e-04 - val_loss: 4.1905e-04\n",
      "Epoch 464/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 4.3626e-04 - val_loss: 4.1893e-04\n",
      "Epoch 465/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 4.3621e-04 - val_loss: 4.1888e-04\n",
      "Epoch 466/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3619e-04 - val_loss: 4.1878e-04\n",
      "Epoch 467/10000\n",
      "378/378 [==============================] - 0s 946us/step - loss: 4.3614e-04 - val_loss: 4.1871e-04\n",
      "Epoch 468/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3613e-04 - val_loss: 4.1860e-04\n",
      "Epoch 469/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.3607e-04 - val_loss: 4.1854e-04\n",
      "Epoch 470/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.3603e-04 - val_loss: 4.1848e-04\n",
      "Epoch 471/10000\n",
      "378/378 [==============================] - 0s 979us/step - loss: 4.3600e-04 - val_loss: 4.1841e-04\n",
      "Epoch 472/10000\n",
      "378/378 [==============================] - 0s 953us/step - loss: 4.3598e-04 - val_loss: 4.1833e-04\n",
      "Epoch 473/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 4.3594e-04 - val_loss: 4.1828e-04\n",
      "Epoch 474/10000\n",
      "378/378 [==============================] - 0s 911us/step - loss: 4.3592e-04 - val_loss: 4.1819e-04\n",
      "Epoch 475/10000\n",
      "378/378 [==============================] - 0s 921us/step - loss: 4.3589e-04 - val_loss: 4.1810e-04\n",
      "Epoch 476/10000\n",
      "378/378 [==============================] - 0s 940us/step - loss: 4.3586e-04 - val_loss: 4.1803e-04\n",
      "Epoch 477/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3583e-04 - val_loss: 4.1796e-04\n",
      "Epoch 478/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.3579e-04 - val_loss: 4.1793e-04\n",
      "Epoch 479/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.3577e-04 - val_loss: 4.1785e-04\n",
      "Epoch 480/10000\n",
      "378/378 [==============================] - 0s 948us/step - loss: 4.3574e-04 - val_loss: 4.1779e-04\n",
      "Epoch 481/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.3572e-04 - val_loss: 4.1772e-04\n",
      "Epoch 482/10000\n",
      "378/378 [==============================] - 0s 932us/step - loss: 4.3569e-04 - val_loss: 4.1767e-04\n",
      "Epoch 483/10000\n",
      "378/378 [==============================] - 0s 968us/step - loss: 4.3567e-04 - val_loss: 4.1761e-04\n",
      "Epoch 484/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 4.3564e-04 - val_loss: 4.1756e-04\n",
      "Epoch 485/10000\n",
      "378/378 [==============================] - 0s 975us/step - loss: 4.3563e-04 - val_loss: 4.1747e-04\n",
      "Epoch 486/10000\n",
      "378/378 [==============================] - 0s 966us/step - loss: 4.3560e-04 - val_loss: 4.1741e-04\n",
      "Epoch 487/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3559e-04 - val_loss: 4.1733e-04\n",
      "Epoch 488/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.3555e-04 - val_loss: 4.1730e-04\n",
      "Epoch 489/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 4.3556e-04 - val_loss: 4.1723e-04\n",
      "Epoch 490/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3552e-04 - val_loss: 4.1720e-04\n",
      "Epoch 491/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 4.3551e-04 - val_loss: 4.1715e-04\n",
      "Epoch 492/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 4.3548e-04 - val_loss: 4.1712e-04\n",
      "Epoch 493/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 4.3548e-04 - val_loss: 4.1706e-04\n",
      "Epoch 494/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3546e-04 - val_loss: 4.1702e-04\n",
      "Epoch 495/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3544e-04 - val_loss: 4.1700e-04\n",
      "Epoch 496/10000\n",
      "378/378 [==============================] - 0s 989us/step - loss: 4.3543e-04 - val_loss: 4.1694e-04\n",
      "Epoch 497/10000\n",
      "378/378 [==============================] - 0s 973us/step - loss: 4.3542e-04 - val_loss: 4.1689e-04\n",
      "Epoch 498/10000\n",
      "378/378 [==============================] - 0s 947us/step - loss: 4.3540e-04 - val_loss: 4.1684e-04\n",
      "Epoch 499/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 4.3538e-04 - val_loss: 4.1681e-04\n",
      "Epoch 500/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 4.3536e-04 - val_loss: 4.1676e-04\n",
      "Epoch 501/10000\n",
      "378/378 [==============================] - 0s 958us/step - loss: 4.3535e-04 - val_loss: 4.1673e-04\n",
      "Epoch 502/10000\n",
      "378/378 [==============================] - 0s 959us/step - loss: 4.3534e-04 - val_loss: 4.1669e-04\n",
      "Epoch 503/10000\n",
      "378/378 [==============================] - 0s 948us/step - loss: 4.3532e-04 - val_loss: 4.1664e-04\n",
      "Epoch 504/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 4.3531e-04 - val_loss: 4.1659e-04\n",
      "Epoch 505/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 4.3529e-04 - val_loss: 4.1655e-04\n",
      "Epoch 506/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.3528e-04 - val_loss: 4.1651e-04\n",
      "Epoch 507/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.3527e-04 - val_loss: 4.1649e-04\n",
      "Epoch 508/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 4.3526e-04 - val_loss: 4.1644e-04\n",
      "Epoch 509/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 4.3525e-04 - val_loss: 4.1641e-04\n",
      "Epoch 510/10000\n",
      "378/378 [==============================] - 0s 991us/step - loss: 4.3524e-04 - val_loss: 4.1635e-04\n",
      "Epoch 511/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3523e-04 - val_loss: 4.1631e-04\n",
      "Epoch 512/10000\n",
      "378/378 [==============================] - 0s 986us/step - loss: 4.3522e-04 - val_loss: 4.1630e-04\n",
      "Epoch 513/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3521e-04 - val_loss: 4.1627e-04\n",
      "Epoch 514/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3520e-04 - val_loss: 4.1623e-04\n",
      "Epoch 515/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.3521e-04 - val_loss: 4.1617e-04\n",
      "Epoch 516/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3517e-04 - val_loss: 4.1615e-04\n",
      "Epoch 517/10000\n",
      "378/378 [==============================] - 0s 929us/step - loss: 4.3517e-04 - val_loss: 4.1612e-04\n",
      "Epoch 518/10000\n",
      "378/378 [==============================] - 0s 925us/step - loss: 4.3516e-04 - val_loss: 4.1609e-04\n",
      "Epoch 519/10000\n",
      "378/378 [==============================] - 0s 965us/step - loss: 4.3516e-04 - val_loss: 4.1607e-04\n",
      "Epoch 520/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 4.3517e-04 - val_loss: 4.1601e-04\n",
      "Epoch 521/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 4.3514e-04 - val_loss: 4.1598e-04\n",
      "Epoch 522/10000\n",
      "378/378 [==============================] - 0s 974us/step - loss: 4.3513e-04 - val_loss: 4.1596e-04\n",
      "Epoch 523/10000\n",
      "378/378 [==============================] - 0s 959us/step - loss: 4.3513e-04 - val_loss: 4.1596e-04\n",
      "Epoch 524/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.3512e-04 - val_loss: 4.1592e-04\n",
      "Epoch 525/10000\n",
      "378/378 [==============================] - 0s 962us/step - loss: 4.3511e-04 - val_loss: 4.1589e-04\n",
      "Epoch 526/10000\n",
      "378/378 [==============================] - 0s 945us/step - loss: 4.3510e-04 - val_loss: 4.1588e-04\n",
      "Epoch 527/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 4.3510e-04 - val_loss: 4.1585e-04\n",
      "Epoch 528/10000\n",
      "378/378 [==============================] - 0s 937us/step - loss: 4.3510e-04 - val_loss: 4.1581e-04\n",
      "Epoch 529/10000\n",
      "378/378 [==============================] - 0s 952us/step - loss: 4.3509e-04 - val_loss: 4.1578e-04\n",
      "Epoch 530/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3508e-04 - val_loss: 4.1578e-04\n",
      "Epoch 531/10000\n",
      "378/378 [==============================] - 0s 922us/step - loss: 4.3507e-04 - val_loss: 4.1575e-04\n",
      "Epoch 532/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3507e-04 - val_loss: 4.1571e-04\n",
      "Epoch 533/10000\n",
      "378/378 [==============================] - 0s 983us/step - loss: 4.3506e-04 - val_loss: 4.1571e-04\n",
      "Epoch 534/10000\n",
      "378/378 [==============================] - 0s 987us/step - loss: 4.3507e-04 - val_loss: 4.1567e-04\n",
      "Epoch 535/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.3506e-04 - val_loss: 4.1564e-04\n",
      "Epoch 536/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 4.3505e-04 - val_loss: 4.1565e-04\n",
      "Epoch 537/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 4.3505e-04 - val_loss: 4.1563e-04\n",
      "Epoch 538/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 4.3504e-04 - val_loss: 4.1561e-04\n",
      "Epoch 539/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3504e-04 - val_loss: 4.1560e-04\n",
      "Epoch 540/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 4.3504e-04 - val_loss: 4.1556e-04\n",
      "Epoch 541/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3503e-04 - val_loss: 4.1557e-04\n",
      "Epoch 542/10000\n",
      "378/378 [==============================] - 0s 934us/step - loss: 4.3503e-04 - val_loss: 4.1555e-04\n",
      "Epoch 543/10000\n",
      "378/378 [==============================] - 0s 955us/step - loss: 4.3502e-04 - val_loss: 4.1552e-04\n",
      "Epoch 544/10000\n",
      "378/378 [==============================] - 0s 978us/step - loss: 4.3502e-04 - val_loss: 4.1549e-04\n",
      "Epoch 545/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 4.3502e-04 - val_loss: 4.1546e-04\n",
      "Epoch 546/10000\n",
      "378/378 [==============================] - 0s 995us/step - loss: 4.3501e-04 - val_loss: 4.1545e-04\n",
      "Epoch 547/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3501e-04 - val_loss: 4.1544e-04\n",
      "Epoch 548/10000\n",
      "378/378 [==============================] - 0s 990us/step - loss: 4.3500e-04 - val_loss: 4.1542e-04\n",
      "Epoch 549/10000\n",
      "378/378 [==============================] - 0s 967us/step - loss: 4.3500e-04 - val_loss: 4.1540e-04\n",
      "Epoch 550/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 4.3500e-04 - val_loss: 4.1538e-04\n",
      "Epoch 551/10000\n",
      "378/378 [==============================] - 0s 950us/step - loss: 4.3499e-04 - val_loss: 4.1537e-04\n",
      "Epoch 552/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 4.3499e-04 - val_loss: 4.1535e-04\n",
      "Epoch 553/10000\n",
      "378/378 [==============================] - 0s 937us/step - loss: 4.3500e-04 - val_loss: 4.1536e-04\n",
      "Epoch 554/10000\n",
      "378/378 [==============================] - 0s 953us/step - loss: 4.3503e-04 - val_loss: 4.1530e-04\n",
      "Epoch 555/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3500e-04 - val_loss: 4.1527e-04\n",
      "Epoch 556/10000\n",
      "378/378 [==============================] - 0s 984us/step - loss: 4.3498e-04 - val_loss: 4.1527e-04\n",
      "Epoch 557/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 4.3498e-04 - val_loss: 4.1526e-04\n",
      "Epoch 558/10000\n",
      "378/378 [==============================] - 0s 970us/step - loss: 4.3498e-04 - val_loss: 4.1525e-04\n",
      "Epoch 559/10000\n",
      "378/378 [==============================] - 0s 982us/step - loss: 4.3498e-04 - val_loss: 4.1524e-04\n",
      "Epoch 560/10000\n",
      "378/378 [==============================] - 0s 996us/step - loss: 4.3498e-04 - val_loss: 4.1522e-04\n",
      "Epoch 561/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.3497e-04 - val_loss: 4.1522e-04\n",
      "Epoch 562/10000\n",
      "378/378 [==============================] - 0s 977us/step - loss: 4.3498e-04 - val_loss: 4.1520e-04\n",
      "Epoch 563/10000\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 4.3497e-04 - val_loss: 4.1518e-04\n",
      "Epoch 564/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.3497e-04 - val_loss: 4.1516e-04\n",
      "Epoch 565/10000\n",
      "378/378 [==============================] - 0s 981us/step - loss: 4.3496e-04 - val_loss: 4.1517e-04\n",
      "Epoch 566/10000\n",
      "378/378 [==============================] - 0s 988us/step - loss: 4.3496e-04 - val_loss: 4.1514e-04\n",
      "Epoch 567/10000\n",
      "378/378 [==============================] - 0s 995us/step - loss: 4.3496e-04 - val_loss: 4.1515e-04\n",
      "Epoch 568/10000\n",
      "378/378 [==============================] - 0s 976us/step - loss: 4.3496e-04 - val_loss: 4.1512e-04\n",
      "Epoch 569/10000\n",
      "378/378 [==============================] - 0s 972us/step - loss: 4.3495e-04 - val_loss: 4.1512e-04\n",
      "Epoch 570/10000\n",
      "378/378 [==============================] - 0s 951us/step - loss: 4.3495e-04 - val_loss: 4.1513e-04\n",
      "Epoch 571/10000\n",
      "378/378 [==============================] - 0s 971us/step - loss: 4.3496e-04 - val_loss: 4.1510e-04\n",
      "Epoch 572/10000\n",
      "378/378 [==============================] - 0s 985us/step - loss: 4.3496e-04 - val_loss: 4.1509e-04\n",
      "Epoch 573/10000\n",
      "378/378 [==============================] - 0s 964us/step - loss: 4.3495e-04 - val_loss: 4.1509e-04\n",
      "Epoch 574/10000\n",
      "378/378 [==============================] - 0s 980us/step - loss: 4.3495e-04 - val_loss: 4.1507e-04\n",
      "Epoch 575/10000\n",
      "378/378 [==============================] - 0s 935us/step - loss: 4.3494e-04 - val_loss: 4.1508e-04\n",
      "Epoch 576/10000\n",
      "378/378 [==============================] - 0s 957us/step - loss: 4.3496e-04 - val_loss: 4.1503e-04\n",
      "Epoch 577/10000\n",
      "378/378 [==============================] - 0s 916us/step - loss: 4.3494e-04 - val_loss: 4.1504e-04\n",
      "Epoch 578/10000\n",
      "378/378 [==============================] - 0s 956us/step - loss: 4.3494e-04 - val_loss: 4.1504e-04\n",
      "Epoch 579/10000\n",
      "378/378 [==============================] - 0s 992us/step - loss: 4.3494e-04 - val_loss: 4.1504e-04\n"
     ]
    }
   ],
   "source": [
    "earlyStopping =keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=0, mode='auto')\n",
    "autoencoder_train = model.fit(X_train,X_train,epochs=10000,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test), callbacks=[earlyStopping])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 411 samples, validate on 103 samples\n",
      "Epoch 1/10000\n",
      "411/411 [==============================] - 1s 2ms/step - loss: 0.2429 - val_loss: 0.2429\n",
      "Epoch 2/10000\n",
      "411/411 [==============================] - 0s 408us/step - loss: 0.2415 - val_loss: 0.2414\n",
      "Epoch 3/10000\n",
      "411/411 [==============================] - 0s 351us/step - loss: 0.2399 - val_loss: 0.2398\n",
      "Epoch 4/10000\n",
      "411/411 [==============================] - 0s 339us/step - loss: 0.2384 - val_loss: 0.2382\n",
      "Epoch 5/10000\n",
      "411/411 [==============================] - 0s 414us/step - loss: 0.2368 - val_loss: 0.2367\n",
      "Epoch 6/10000\n",
      "411/411 [==============================] - 0s 427us/step - loss: 0.2352 - val_loss: 0.2350\n",
      "Epoch 7/10000\n",
      "411/411 [==============================] - 0s 474us/step - loss: 0.2336 - val_loss: 0.2334\n",
      "Epoch 8/10000\n",
      "411/411 [==============================] - 0s 428us/step - loss: 0.2319 - val_loss: 0.2317\n",
      "Epoch 9/10000\n",
      "411/411 [==============================] - 0s 409us/step - loss: 0.2302 - val_loss: 0.2300\n",
      "Epoch 10/10000\n",
      "411/411 [==============================] - 0s 354us/step - loss: 0.2284 - val_loss: 0.2282\n",
      "Epoch 11/10000\n",
      "411/411 [==============================] - 0s 377us/step - loss: 0.2266 - val_loss: 0.2264\n",
      "Epoch 12/10000\n",
      "411/411 [==============================] - 0s 391us/step - loss: 0.2248 - val_loss: 0.2245\n",
      "Epoch 13/10000\n",
      "411/411 [==============================] - 0s 426us/step - loss: 0.2229 - val_loss: 0.2226\n",
      "Epoch 14/10000\n",
      "411/411 [==============================] - 0s 382us/step - loss: 0.2210 - val_loss: 0.2206\n",
      "Epoch 15/10000\n",
      "411/411 [==============================] - 0s 380us/step - loss: 0.2190 - val_loss: 0.2186\n",
      "Epoch 16/10000\n",
      "411/411 [==============================] - 0s 381us/step - loss: 0.2170 - val_loss: 0.2165\n",
      "Epoch 17/10000\n",
      "411/411 [==============================] - 0s 425us/step - loss: 0.2149 - val_loss: 0.2144\n",
      "Epoch 18/10000\n",
      "411/411 [==============================] - 0s 385us/step - loss: 0.2128 - val_loss: 0.2123\n",
      "Epoch 19/10000\n",
      "411/411 [==============================] - 0s 407us/step - loss: 0.2107 - val_loss: 0.2102\n",
      "Epoch 20/10000\n",
      "411/411 [==============================] - 0s 422us/step - loss: 0.2085 - val_loss: 0.2080\n",
      "Epoch 21/10000\n",
      "411/411 [==============================] - 0s 398us/step - loss: 0.2063 - val_loss: 0.2057\n",
      "Epoch 22/10000\n",
      "411/411 [==============================] - 0s 388us/step - loss: 0.2040 - val_loss: 0.2035\n",
      "Epoch 23/10000\n",
      "411/411 [==============================] - 0s 342us/step - loss: 0.2017 - val_loss: 0.2012\n",
      "Epoch 24/10000\n",
      "411/411 [==============================] - 0s 409us/step - loss: 0.1994 - val_loss: 0.1988\n",
      "Epoch 25/10000\n",
      "411/411 [==============================] - 0s 443us/step - loss: 0.1971 - val_loss: 0.1965\n",
      "Epoch 26/10000\n",
      "411/411 [==============================] - 0s 431us/step - loss: 0.1948 - val_loss: 0.1941\n",
      "Epoch 27/10000\n",
      "411/411 [==============================] - 0s 392us/step - loss: 0.1924 - val_loss: 0.1917\n",
      "Epoch 28/10000\n",
      "411/411 [==============================] - 0s 437us/step - loss: 0.1900 - val_loss: 0.1893\n",
      "Epoch 29/10000\n",
      "411/411 [==============================] - 0s 419us/step - loss: 0.1876 - val_loss: 0.1869\n",
      "Epoch 30/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.1852 - val_loss: 0.1845\n",
      "Epoch 31/10000\n",
      "411/411 [==============================] - 0s 392us/step - loss: 0.1827 - val_loss: 0.1820\n",
      "Epoch 32/10000\n",
      "411/411 [==============================] - 0s 362us/step - loss: 0.1803 - val_loss: 0.1796\n",
      "Epoch 33/10000\n",
      "411/411 [==============================] - 0s 379us/step - loss: 0.1778 - val_loss: 0.1771\n",
      "Epoch 34/10000\n",
      "411/411 [==============================] - 0s 376us/step - loss: 0.1753 - val_loss: 0.1746\n",
      "Epoch 35/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.1728 - val_loss: 0.1721\n",
      "Epoch 36/10000\n",
      "411/411 [==============================] - 0s 437us/step - loss: 0.1704 - val_loss: 0.1696\n",
      "Epoch 37/10000\n",
      "411/411 [==============================] - 0s 453us/step - loss: 0.1679 - val_loss: 0.1671\n",
      "Epoch 38/10000\n",
      "411/411 [==============================] - 0s 448us/step - loss: 0.1654 - val_loss: 0.1646\n",
      "Epoch 39/10000\n",
      "411/411 [==============================] - 0s 428us/step - loss: 0.1629 - val_loss: 0.1621\n",
      "Epoch 40/10000\n",
      "411/411 [==============================] - 0s 440us/step - loss: 0.1604 - val_loss: 0.1596\n",
      "Epoch 41/10000\n",
      "411/411 [==============================] - 0s 435us/step - loss: 0.1579 - val_loss: 0.1571\n",
      "Epoch 42/10000\n",
      "411/411 [==============================] - 0s 430us/step - loss: 0.1554 - val_loss: 0.1546\n",
      "Epoch 43/10000\n",
      "411/411 [==============================] - 0s 450us/step - loss: 0.1529 - val_loss: 0.1522\n",
      "Epoch 44/10000\n",
      "411/411 [==============================] - 0s 437us/step - loss: 0.1505 - val_loss: 0.1497\n",
      "Epoch 45/10000\n",
      "411/411 [==============================] - 0s 377us/step - loss: 0.1480 - val_loss: 0.1472\n",
      "Epoch 46/10000\n",
      "411/411 [==============================] - 0s 373us/step - loss: 0.1456 - val_loss: 0.1448\n",
      "Epoch 47/10000\n",
      "411/411 [==============================] - 0s 404us/step - loss: 0.1431 - val_loss: 0.1424\n",
      "Epoch 48/10000\n",
      "411/411 [==============================] - 0s 416us/step - loss: 0.1407 - val_loss: 0.1399\n",
      "Epoch 49/10000\n",
      "411/411 [==============================] - 0s 404us/step - loss: 0.1383 - val_loss: 0.1375\n",
      "Epoch 50/10000\n",
      "411/411 [==============================] - 0s 419us/step - loss: 0.1359 - val_loss: 0.1351\n",
      "Epoch 51/10000\n",
      "411/411 [==============================] - 0s 377us/step - loss: 0.1335 - val_loss: 0.1328\n",
      "Epoch 52/10000\n",
      "411/411 [==============================] - 0s 393us/step - loss: 0.1312 - val_loss: 0.1304\n",
      "Epoch 53/10000\n",
      "411/411 [==============================] - 0s 427us/step - loss: 0.1288 - val_loss: 0.1281\n",
      "Epoch 54/10000\n",
      "411/411 [==============================] - 0s 425us/step - loss: 0.1265 - val_loss: 0.1258\n",
      "Epoch 55/10000\n",
      "411/411 [==============================] - 0s 401us/step - loss: 0.1242 - val_loss: 0.1235\n",
      "Epoch 56/10000\n",
      "411/411 [==============================] - 0s 395us/step - loss: 0.1219 - val_loss: 0.1212\n",
      "Epoch 57/10000\n",
      "411/411 [==============================] - 0s 350us/step - loss: 0.1197 - val_loss: 0.1190\n",
      "Epoch 58/10000\n",
      "411/411 [==============================] - 0s 368us/step - loss: 0.1175 - val_loss: 0.1168\n",
      "Epoch 59/10000\n",
      "411/411 [==============================] - 0s 424us/step - loss: 0.1153 - val_loss: 0.1146\n",
      "Epoch 60/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.1131 - val_loss: 0.1124\n",
      "Epoch 61/10000\n",
      "411/411 [==============================] - 0s 426us/step - loss: 0.1109 - val_loss: 0.1103\n",
      "Epoch 62/10000\n",
      "411/411 [==============================] - 0s 396us/step - loss: 0.1088 - val_loss: 0.1082\n",
      "Epoch 63/10000\n",
      "411/411 [==============================] - 0s 360us/step - loss: 0.1067 - val_loss: 0.1061\n",
      "Epoch 64/10000\n",
      "411/411 [==============================] - 0s 351us/step - loss: 0.1047 - val_loss: 0.1040\n",
      "Epoch 65/10000\n",
      "411/411 [==============================] - 0s 400us/step - loss: 0.1026 - val_loss: 0.1020\n",
      "Epoch 66/10000\n",
      "411/411 [==============================] - 0s 398us/step - loss: 0.1006 - val_loss: 0.1000\n",
      "Epoch 67/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0986 - val_loss: 0.0980\n",
      "Epoch 68/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0967 - val_loss: 0.0961\n",
      "Epoch 69/10000\n",
      "411/411 [==============================] - 0s 411us/step - loss: 0.0948 - val_loss: 0.0942\n",
      "Epoch 70/10000\n",
      "411/411 [==============================] - 0s 436us/step - loss: 0.0929 - val_loss: 0.0923\n",
      "Epoch 71/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0910 - val_loss: 0.0904\n",
      "Epoch 72/10000\n",
      "411/411 [==============================] - 0s 388us/step - loss: 0.0892 - val_loss: 0.0886\n",
      "Epoch 73/10000\n",
      "411/411 [==============================] - 0s 406us/step - loss: 0.0874 - val_loss: 0.0868\n",
      "Epoch 74/10000\n",
      "411/411 [==============================] - 0s 387us/step - loss: 0.0856 - val_loss: 0.0851\n",
      "Epoch 75/10000\n",
      "411/411 [==============================] - 0s 396us/step - loss: 0.0839 - val_loss: 0.0834\n",
      "Epoch 76/10000\n",
      "411/411 [==============================] - 0s 377us/step - loss: 0.0822 - val_loss: 0.0817\n",
      "Epoch 77/10000\n",
      "411/411 [==============================] - 0s 396us/step - loss: 0.0805 - val_loss: 0.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/10000\n",
      "411/411 [==============================] - 0s 421us/step - loss: 0.0788 - val_loss: 0.0784\n",
      "Epoch 79/10000\n",
      "411/411 [==============================] - 0s 414us/step - loss: 0.0772 - val_loss: 0.0768\n",
      "Epoch 80/10000\n",
      "411/411 [==============================] - 0s 449us/step - loss: 0.0756 - val_loss: 0.0752\n",
      "Epoch 81/10000\n",
      "411/411 [==============================] - 0s 388us/step - loss: 0.0741 - val_loss: 0.0736\n",
      "Epoch 82/10000\n",
      "411/411 [==============================] - 0s 348us/step - loss: 0.0725 - val_loss: 0.0721\n",
      "Epoch 83/10000\n",
      "411/411 [==============================] - 0s 399us/step - loss: 0.0710 - val_loss: 0.0706\n",
      "Epoch 84/10000\n",
      "411/411 [==============================] - 0s 385us/step - loss: 0.0696 - val_loss: 0.0692\n",
      "Epoch 85/10000\n",
      "411/411 [==============================] - 0s 375us/step - loss: 0.0681 - val_loss: 0.0677\n",
      "Epoch 86/10000\n",
      "411/411 [==============================] - 0s 417us/step - loss: 0.0667 - val_loss: 0.0663\n",
      "Epoch 87/10000\n",
      "411/411 [==============================] - 0s 402us/step - loss: 0.0653 - val_loss: 0.0650\n",
      "Epoch 88/10000\n",
      "411/411 [==============================] - 0s 428us/step - loss: 0.0640 - val_loss: 0.0636\n",
      "Epoch 89/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.0627 - val_loss: 0.0623\n",
      "Epoch 90/10000\n",
      "411/411 [==============================] - 0s 394us/step - loss: 0.0614 - val_loss: 0.0610\n",
      "Epoch 91/10000\n",
      "411/411 [==============================] - 0s 441us/step - loss: 0.0601 - val_loss: 0.0598\n",
      "Epoch 92/10000\n",
      "411/411 [==============================] - 0s 406us/step - loss: 0.0588 - val_loss: 0.0585\n",
      "Epoch 93/10000\n",
      "411/411 [==============================] - 0s 386us/step - loss: 0.0576 - val_loss: 0.0573\n",
      "Epoch 94/10000\n",
      "411/411 [==============================] - 0s 375us/step - loss: 0.0564 - val_loss: 0.0561\n",
      "Epoch 95/10000\n",
      "411/411 [==============================] - 0s 360us/step - loss: 0.0553 - val_loss: 0.0550\n",
      "Epoch 96/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0541 - val_loss: 0.0539\n",
      "Epoch 97/10000\n",
      "411/411 [==============================] - 0s 354us/step - loss: 0.0530 - val_loss: 0.0527\n",
      "Epoch 98/10000\n",
      "411/411 [==============================] - 0s 355us/step - loss: 0.0519 - val_loss: 0.0517\n",
      "Epoch 99/10000\n",
      "411/411 [==============================] - 0s 350us/step - loss: 0.0508 - val_loss: 0.0506\n",
      "Epoch 100/10000\n",
      "411/411 [==============================] - 0s 341us/step - loss: 0.0498 - val_loss: 0.0496\n",
      "Epoch 101/10000\n",
      "411/411 [==============================] - 0s 336us/step - loss: 0.0488 - val_loss: 0.0486\n",
      "Epoch 102/10000\n",
      "411/411 [==============================] - 0s 379us/step - loss: 0.0478 - val_loss: 0.0476\n",
      "Epoch 103/10000\n",
      "411/411 [==============================] - 0s 432us/step - loss: 0.0468 - val_loss: 0.0466\n",
      "Epoch 104/10000\n",
      "411/411 [==============================] - 0s 389us/step - loss: 0.0458 - val_loss: 0.0457\n",
      "Epoch 105/10000\n",
      "411/411 [==============================] - 0s 383us/step - loss: 0.0449 - val_loss: 0.0447\n",
      "Epoch 106/10000\n",
      "411/411 [==============================] - 0s 361us/step - loss: 0.0440 - val_loss: 0.0438\n",
      "Epoch 107/10000\n",
      "411/411 [==============================] - 0s 385us/step - loss: 0.0431 - val_loss: 0.0429\n",
      "Epoch 108/10000\n",
      "411/411 [==============================] - 0s 451us/step - loss: 0.0422 - val_loss: 0.0421\n",
      "Epoch 109/10000\n",
      "411/411 [==============================] - 0s 448us/step - loss: 0.0414 - val_loss: 0.0412\n",
      "Epoch 110/10000\n",
      "411/411 [==============================] - 0s 394us/step - loss: 0.0406 - val_loss: 0.0404\n",
      "Epoch 111/10000\n",
      "411/411 [==============================] - 0s 372us/step - loss: 0.0397 - val_loss: 0.0396\n",
      "Epoch 112/10000\n",
      "411/411 [==============================] - 0s 398us/step - loss: 0.0389 - val_loss: 0.0388\n",
      "Epoch 113/10000\n",
      "411/411 [==============================] - 0s 388us/step - loss: 0.0382 - val_loss: 0.0380\n",
      "Epoch 114/10000\n",
      "411/411 [==============================] - 0s 399us/step - loss: 0.0374 - val_loss: 0.0373\n",
      "Epoch 115/10000\n",
      "411/411 [==============================] - 0s 403us/step - loss: 0.0367 - val_loss: 0.0366\n",
      "Epoch 116/10000\n",
      "411/411 [==============================] - 0s 420us/step - loss: 0.0359 - val_loss: 0.0358\n",
      "Epoch 117/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0352 - val_loss: 0.0351\n",
      "Epoch 118/10000\n",
      "411/411 [==============================] - 0s 432us/step - loss: 0.0345 - val_loss: 0.0345\n",
      "Epoch 119/10000\n",
      "411/411 [==============================] - 0s 371us/step - loss: 0.0339 - val_loss: 0.0338\n",
      "Epoch 120/10000\n",
      "411/411 [==============================] - 0s 347us/step - loss: 0.0332 - val_loss: 0.0331\n",
      "Epoch 121/10000\n",
      "411/411 [==============================] - 0s 431us/step - loss: 0.0326 - val_loss: 0.0325\n",
      "Epoch 122/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 123/10000\n",
      "411/411 [==============================] - 0s 387us/step - loss: 0.0313 - val_loss: 0.0313\n",
      "Epoch 124/10000\n",
      "411/411 [==============================] - 0s 392us/step - loss: 0.0307 - val_loss: 0.0307\n",
      "Epoch 125/10000\n",
      "411/411 [==============================] - 0s 361us/step - loss: 0.0301 - val_loss: 0.0301\n",
      "Epoch 126/10000\n",
      "411/411 [==============================] - 0s 328us/step - loss: 0.0296 - val_loss: 0.0295\n",
      "Epoch 127/10000\n",
      "411/411 [==============================] - 0s 373us/step - loss: 0.0290 - val_loss: 0.0289\n",
      "Epoch 128/10000\n",
      "411/411 [==============================] - 0s 375us/step - loss: 0.0284 - val_loss: 0.0284\n",
      "Epoch 129/10000\n",
      "411/411 [==============================] - 0s 379us/step - loss: 0.0279 - val_loss: 0.0279\n",
      "Epoch 130/10000\n",
      "411/411 [==============================] - 0s 393us/step - loss: 0.0274 - val_loss: 0.0274\n",
      "Epoch 131/10000\n",
      "411/411 [==============================] - 0s 418us/step - loss: 0.0269 - val_loss: 0.0268\n",
      "Epoch 132/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.0264 - val_loss: 0.0264\n",
      "Epoch 133/10000\n",
      "411/411 [==============================] - 0s 396us/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 134/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 135/10000\n",
      "411/411 [==============================] - 0s 398us/step - loss: 0.0249 - val_loss: 0.0249\n",
      "Epoch 136/10000\n",
      "411/411 [==============================] - 0s 430us/step - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 137/10000\n",
      "411/411 [==============================] - 0s 422us/step - loss: 0.0240 - val_loss: 0.0240\n",
      "Epoch 138/10000\n",
      "411/411 [==============================] - 0s 373us/step - loss: 0.0236 - val_loss: 0.0236\n",
      "Epoch 139/10000\n",
      "411/411 [==============================] - 0s 434us/step - loss: 0.0232 - val_loss: 0.0232\n",
      "Epoch 140/10000\n",
      "411/411 [==============================] - 0s 407us/step - loss: 0.0228 - val_loss: 0.0228\n",
      "Epoch 141/10000\n",
      "411/411 [==============================] - 0s 431us/step - loss: 0.0224 - val_loss: 0.0224\n",
      "Epoch 142/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.0220 - val_loss: 0.0220\n",
      "Epoch 143/10000\n",
      "411/411 [==============================] - 0s 377us/step - loss: 0.0216 - val_loss: 0.0216\n",
      "Epoch 144/10000\n",
      "411/411 [==============================] - 0s 350us/step - loss: 0.0212 - val_loss: 0.0212\n",
      "Epoch 145/10000\n",
      "411/411 [==============================] - 0s 372us/step - loss: 0.0208 - val_loss: 0.0208\n",
      "Epoch 146/10000\n",
      "411/411 [==============================] - 0s 398us/step - loss: 0.0205 - val_loss: 0.0205\n",
      "Epoch 147/10000\n",
      "411/411 [==============================] - 0s 399us/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 148/10000\n",
      "411/411 [==============================] - 0s 393us/step - loss: 0.0198 - val_loss: 0.0198\n",
      "Epoch 149/10000\n",
      "411/411 [==============================] - 0s 384us/step - loss: 0.0194 - val_loss: 0.0194\n",
      "Epoch 150/10000\n",
      "411/411 [==============================] - 0s 396us/step - loss: 0.0191 - val_loss: 0.0191\n",
      "Epoch 151/10000\n",
      "411/411 [==============================] - 0s 361us/step - loss: 0.0188 - val_loss: 0.0188\n",
      "Epoch 152/10000\n",
      "411/411 [==============================] - 0s 401us/step - loss: 0.0184 - val_loss: 0.0185\n",
      "Epoch 153/10000\n",
      "411/411 [==============================] - 0s 420us/step - loss: 0.0181 - val_loss: 0.0182\n",
      "Epoch 154/10000\n",
      "411/411 [==============================] - 0s 449us/step - loss: 0.0178 - val_loss: 0.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/10000\n",
      "411/411 [==============================] - 0s 437us/step - loss: 0.0175 - val_loss: 0.0176\n",
      "Epoch 156/10000\n",
      "411/411 [==============================] - 0s 389us/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 157/10000\n",
      "411/411 [==============================] - 0s 355us/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 158/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 159/10000\n",
      "411/411 [==============================] - 0s 372us/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 160/10000\n",
      "411/411 [==============================] - 0s 394us/step - loss: 0.0161 - val_loss: 0.0162\n",
      "Epoch 161/10000\n",
      "411/411 [==============================] - 0s 432us/step - loss: 0.0159 - val_loss: 0.0159\n",
      "Epoch 162/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 163/10000\n",
      "411/411 [==============================] - 0s 359us/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 164/10000\n",
      "411/411 [==============================] - 0s 413us/step - loss: 0.0151 - val_loss: 0.0152\n",
      "Epoch 165/10000\n",
      "411/411 [==============================] - 0s 443us/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 166/10000\n",
      "411/411 [==============================] - 0s 360us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 167/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 168/10000\n",
      "411/411 [==============================] - 0s 423us/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 169/10000\n",
      "411/411 [==============================] - 0s 406us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 170/10000\n",
      "411/411 [==============================] - 0s 415us/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 171/10000\n",
      "411/411 [==============================] - 0s 443us/step - loss: 0.0135 - val_loss: 0.0136\n",
      "Epoch 172/10000\n",
      "411/411 [==============================] - 0s 452us/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 173/10000\n",
      "411/411 [==============================] - 0s 381us/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 174/10000\n",
      "411/411 [==============================] - 0s 368us/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 175/10000\n",
      "411/411 [==============================] - 0s 355us/step - loss: 0.0127 - val_loss: 0.0128\n",
      "Epoch 176/10000\n",
      "411/411 [==============================] - 0s 371us/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 177/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0123 - val_loss: 0.0124\n",
      "Epoch 178/10000\n",
      "411/411 [==============================] - 0s 381us/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 179/10000\n",
      "411/411 [==============================] - 0s 369us/step - loss: 0.0120 - val_loss: 0.0121\n",
      "Epoch 180/10000\n",
      "411/411 [==============================] - 0s 383us/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 181/10000\n",
      "411/411 [==============================] - 0s 376us/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 182/10000\n",
      "411/411 [==============================] - 0s 402us/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 183/10000\n",
      "411/411 [==============================] - 0s 342us/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 184/10000\n",
      "411/411 [==============================] - 0s 394us/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 185/10000\n",
      "411/411 [==============================] - 0s 421us/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 186/10000\n",
      "411/411 [==============================] - 0s 413us/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 187/10000\n",
      "411/411 [==============================] - 0s 409us/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 188/10000\n",
      "411/411 [==============================] - 0s 399us/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 189/10000\n",
      "411/411 [==============================] - 0s 405us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 190/10000\n",
      "411/411 [==============================] - 0s 451us/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 191/10000\n",
      "411/411 [==============================] - 0s 442us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 192/10000\n",
      "411/411 [==============================] - 0s 427us/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 193/10000\n",
      "411/411 [==============================] - 0s 389us/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 194/10000\n",
      "411/411 [==============================] - 0s 416us/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 195/10000\n",
      "411/411 [==============================] - 0s 422us/step - loss: 0.0095 - val_loss: 0.0096\n",
      "Epoch 196/10000\n",
      "411/411 [==============================] - 0s 389us/step - loss: 0.0094 - val_loss: 0.0095\n",
      "Epoch 197/10000\n",
      "411/411 [==============================] - 0s 382us/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 198/10000\n",
      "411/411 [==============================] - 0s 380us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 199/10000\n",
      "411/411 [==============================] - 0s 425us/step - loss: 0.0090 - val_loss: 0.0091\n",
      "Epoch 200/10000\n",
      "411/411 [==============================] - 0s 431us/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 201/10000\n",
      "411/411 [==============================] - 0s 401us/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 202/10000\n",
      " 32/411 [=>............................] - ETA: 0s - loss: 0.0088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0d3eff1104f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 validation_data=(X_test, X_test), callbacks=[earlyStopping])\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhid1Neurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid2Neurons\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhid1Neurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid2Neurons\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = np.zeros((500,40))\n",
    "val_loss = np.zeros((500,40))\n",
    "\n",
    "for hid1Neurons in range (1, 5000, 10 ):\n",
    "    for hid2Neurons in range (1,200,5):\n",
    "        input = Input(shape = (1, 18977))\n",
    "        x = Dense(hid1Neurons,kernel_initializer='glorot_uniform', activation = 'relu')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        #50 #70:0.0163 #100: 0.0167 #120:0.0171 #150: 0.0159 #5: 0.0201 #25:0.0257 #batchSize =15 , 500:0.0204\n",
    "        bottleneck = Dense(hid2Neurons, kernel_initializer='glorot_uniform', activation = 'relu', name='bottleneck')(x) \n",
    "        ######################################################################################################\n",
    "        x = Dense(hid2Neurons, kernel_initializer='glorot_uniform', activation = 'relu')(bottleneck)\n",
    "        x = Dense(hid1Neurons, kernel_initializer='glorot_uniform', activation = 'relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        out = Dense(18977, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n",
    "        model = Model(input, out)\n",
    "        \n",
    "        #######################################################################################################\n",
    "        sgd = keras.optimizers.SGD(lr=0.005,decay=10e-6,momentum=0.9, nesterov=1)\n",
    "        model.compile(optimizer=adam,\n",
    "              loss='mse')\n",
    "        #######################################################################################################\n",
    "        earlyStopping =keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=6,\n",
    "                              verbose=0, mode='auto')\n",
    "        autoencoder_train = model.fit(X_train,X_train,epochs=10000,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test), callbacks=[earlyStopping])\n",
    "        loss[hid1Neurons,hid2Neurons] = autoencoder_train.history['loss']\n",
    "        val_loss[hid1Neurons,hid2Neurons] = autoencoder_train.history['val_loss']\n",
    "        print(\"Number of neurons of 1st layer:{a},2d layer:{b}, Loss is:{c} \".format(a =hid1Neurons, b = hid2Neurons,\n",
    "                                                                                    c =loss[hid1Neurons,hid2Neurons] ))\n",
    "        print(\"Number of neurons of 1st layer:{a},2d layer:{b}, validation Loss is:{c} \".format(a =hid1Neurons, b = hid2Neurons,\n",
    "                                                                                    c =val_loss[hid1Neurons,hid2Neurons] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Losses ( Validation and training ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "epochs = \n",
    "loss = autoencoder_train.history['loss']\n",
    "np.shape(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "epochs = range(epochs)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Model(inputs=model.input, outputs=model.get_layer('bottleneck').output)\n",
    "Y = m2.predict(patients)\n",
    "Y =np.reshape(Y , (473, 3))\n",
    "df = pd.DataFrame(Y)\n",
    "df.to_csv(\"3CNV.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.405365</td>\n",
       "      <td>0.969846</td>\n",
       "      <td>4.181699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.177290</td>\n",
       "      <td>2.310748</td>\n",
       "      <td>10.639936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.221000</td>\n",
       "      <td>2.593643</td>\n",
       "      <td>10.387173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.393581</td>\n",
       "      <td>1.054199</td>\n",
       "      <td>4.238043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.480137</td>\n",
       "      <td>1.004355</td>\n",
       "      <td>4.121332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.377229</td>\n",
       "      <td>1.017424</td>\n",
       "      <td>4.137586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.518858</td>\n",
       "      <td>0.913558</td>\n",
       "      <td>4.099765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.432282</td>\n",
       "      <td>0.971981</td>\n",
       "      <td>4.137338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.365426</td>\n",
       "      <td>1.003560</td>\n",
       "      <td>4.296404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.132012</td>\n",
       "      <td>2.461014</td>\n",
       "      <td>10.486511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.163760</td>\n",
       "      <td>2.643800</td>\n",
       "      <td>10.367880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.233347</td>\n",
       "      <td>2.373246</td>\n",
       "      <td>10.553312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.461447</td>\n",
       "      <td>0.989706</td>\n",
       "      <td>4.184808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.487524</td>\n",
       "      <td>1.079342</td>\n",
       "      <td>4.104875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.424525</td>\n",
       "      <td>0.980810</td>\n",
       "      <td>4.285163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.418721</td>\n",
       "      <td>0.971976</td>\n",
       "      <td>4.078759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.482930</td>\n",
       "      <td>0.996563</td>\n",
       "      <td>4.066217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.465492</td>\n",
       "      <td>0.886587</td>\n",
       "      <td>4.137391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.410826</td>\n",
       "      <td>1.009122</td>\n",
       "      <td>4.137216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.521340</td>\n",
       "      <td>0.924539</td>\n",
       "      <td>4.218080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.478374</td>\n",
       "      <td>0.838418</td>\n",
       "      <td>4.212677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.482117</td>\n",
       "      <td>0.939063</td>\n",
       "      <td>4.099749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.398954</td>\n",
       "      <td>1.004510</td>\n",
       "      <td>4.116438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.307792</td>\n",
       "      <td>1.030291</td>\n",
       "      <td>4.201726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.432387</td>\n",
       "      <td>1.148121</td>\n",
       "      <td>4.179217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.425002</td>\n",
       "      <td>0.934577</td>\n",
       "      <td>4.202219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.390827</td>\n",
       "      <td>0.963963</td>\n",
       "      <td>4.039243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>6.159369</td>\n",
       "      <td>2.401288</td>\n",
       "      <td>10.242359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>6.276035</td>\n",
       "      <td>2.510397</td>\n",
       "      <td>10.467653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2.381315</td>\n",
       "      <td>1.077981</td>\n",
       "      <td>4.284558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2.598095</td>\n",
       "      <td>0.993276</td>\n",
       "      <td>4.175835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2.455189</td>\n",
       "      <td>1.022071</td>\n",
       "      <td>4.158135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2.379164</td>\n",
       "      <td>1.007111</td>\n",
       "      <td>4.139530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2.365659</td>\n",
       "      <td>0.981777</td>\n",
       "      <td>4.110740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2.468996</td>\n",
       "      <td>0.934858</td>\n",
       "      <td>4.175897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>2.373020</td>\n",
       "      <td>0.885604</td>\n",
       "      <td>4.136834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2.424712</td>\n",
       "      <td>1.048276</td>\n",
       "      <td>4.105114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2.506884</td>\n",
       "      <td>0.939644</td>\n",
       "      <td>4.328193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2.486878</td>\n",
       "      <td>1.019053</td>\n",
       "      <td>4.009541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2.452590</td>\n",
       "      <td>0.928894</td>\n",
       "      <td>4.222302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2.422853</td>\n",
       "      <td>1.047245</td>\n",
       "      <td>4.129808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2.381847</td>\n",
       "      <td>1.002432</td>\n",
       "      <td>4.208905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2.550461</td>\n",
       "      <td>0.961810</td>\n",
       "      <td>4.105996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2.417558</td>\n",
       "      <td>0.917465</td>\n",
       "      <td>4.136685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2.438959</td>\n",
       "      <td>1.079058</td>\n",
       "      <td>4.151726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2.365868</td>\n",
       "      <td>1.039985</td>\n",
       "      <td>4.139885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2.339172</td>\n",
       "      <td>0.976357</td>\n",
       "      <td>4.102302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2.457757</td>\n",
       "      <td>1.042000</td>\n",
       "      <td>4.034942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2.362551</td>\n",
       "      <td>0.929846</td>\n",
       "      <td>4.270555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2.468372</td>\n",
       "      <td>0.926156</td>\n",
       "      <td>4.123630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>2.450813</td>\n",
       "      <td>0.949869</td>\n",
       "      <td>4.199376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2.547789</td>\n",
       "      <td>0.946464</td>\n",
       "      <td>4.196218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2.415458</td>\n",
       "      <td>1.131096</td>\n",
       "      <td>4.108447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2.442862</td>\n",
       "      <td>0.883039</td>\n",
       "      <td>4.196183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2.430656</td>\n",
       "      <td>1.019450</td>\n",
       "      <td>4.118804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2\n",
       "0    0.000000  0.000000   0.000000\n",
       "1    0.000000  0.000000   0.000000\n",
       "2    2.405365  0.969846   4.181699\n",
       "3    6.177290  2.310748  10.639936\n",
       "4    6.221000  2.593643  10.387173\n",
       "5    2.393581  1.054199   4.238043\n",
       "6    2.480137  1.004355   4.121332\n",
       "7    2.377229  1.017424   4.137586\n",
       "8    2.518858  0.913558   4.099765\n",
       "9    2.432282  0.971981   4.137338\n",
       "10   2.365426  1.003560   4.296404\n",
       "11   6.132012  2.461014  10.486511\n",
       "12   6.163760  2.643800  10.367880\n",
       "13   6.233347  2.373246  10.553312\n",
       "14   2.461447  0.989706   4.184808\n",
       "15   2.487524  1.079342   4.104875\n",
       "16   2.424525  0.980810   4.285163\n",
       "17   2.418721  0.971976   4.078759\n",
       "18   2.482930  0.996563   4.066217\n",
       "19   2.465492  0.886587   4.137391\n",
       "20   2.410826  1.009122   4.137216\n",
       "21   2.521340  0.924539   4.218080\n",
       "22   2.478374  0.838418   4.212677\n",
       "23   2.482117  0.939063   4.099749\n",
       "24   0.000000  0.000000   0.000000\n",
       "25   2.398954  1.004510   4.116438\n",
       "26   2.307792  1.030291   4.201726\n",
       "27   2.432387  1.148121   4.179217\n",
       "28   2.425002  0.934577   4.202219\n",
       "29   2.390827  0.963963   4.039243\n",
       "..        ...       ...        ...\n",
       "443  6.159369  2.401288  10.242359\n",
       "444  6.276035  2.510397  10.467653\n",
       "445  2.381315  1.077981   4.284558\n",
       "446  2.598095  0.993276   4.175835\n",
       "447  2.455189  1.022071   4.158135\n",
       "448  2.379164  1.007111   4.139530\n",
       "449  2.365659  0.981777   4.110740\n",
       "450  2.468996  0.934858   4.175897\n",
       "451  2.373020  0.885604   4.136834\n",
       "452  2.424712  1.048276   4.105114\n",
       "453  2.506884  0.939644   4.328193\n",
       "454  2.486878  1.019053   4.009541\n",
       "455  2.452590  0.928894   4.222302\n",
       "456  2.422853  1.047245   4.129808\n",
       "457  2.381847  1.002432   4.208905\n",
       "458  2.550461  0.961810   4.105996\n",
       "459  2.417558  0.917465   4.136685\n",
       "460  2.438959  1.079058   4.151726\n",
       "461  2.365868  1.039985   4.139885\n",
       "462  2.339172  0.976357   4.102302\n",
       "463  2.457757  1.042000   4.034942\n",
       "464  0.000000  0.000000   0.000000\n",
       "465  2.362551  0.929846   4.270555\n",
       "466  2.468372  0.926156   4.123630\n",
       "467  2.450813  0.949869   4.199376\n",
       "468  2.547789  0.946464   4.196218\n",
       "469  2.415458  1.131096   4.108447\n",
       "470  2.442862  0.883039   4.196183\n",
       "471  2.430656  1.019450   4.118804\n",
       "472  0.000000  0.000000   0.000000\n",
       "\n",
       "[473 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=pd.read_csv(\"3CNV.csv\")\n",
    "np.shape(p)\n",
    "pd.DataFrame(p)\n",
    "#p1 = pd.read_csv(\"500features.csv\")\n",
    "#mean_squared_error(y_true= p , y_pred= p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
